{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa3f239d",
   "metadata": {},
   "source": [
    "\n",
    "# VisA PCB \u2014 Unsupervised Anomaly **Segmentation** Demo (Video + Inference)\n",
    "\n",
    "This notebook helps you:\n",
    "1. **Assemble a demo video** from your **VisA PCB** dataset images (MVTec-style folders) with a simple assembly-line animation.\n",
    "2. **Run your trained UniAS model** on that video at fixed time intervals, saving **masked overlays** and a **CSV log** for operator review.\n",
    "3. (Optional) If your dataset is missing in the environment, it will create a **synthetic test video** so you can test the pipeline end-to-end.\n",
    "\n",
    "> \u2705 Replace `CHECKPOINT_PATH` with your UniAS weights.  \n",
    "> \u2705 Point `DATASET_ROOT` to your prepared VisA **PCB** subset (e.g., `.../VisA/pcb1/`).  \n",
    "> \u2705 The notebook is self-contained and uses OpenCV + PyTorch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102c27c3",
   "metadata": {},
   "source": [
    "## 0) Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07da9e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OpenCV: 4.12.0\n"
     ]
    }
   ],
   "source": [
    "# If needed in your environment (skip if already installed):\n",
    "# %pip install opencv-python torch torchvision numpy matplotlib\n",
    "import os, sys, glob, csv, time, math, random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "\n",
    "try:\n",
    "    import torch\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"PyTorch is required for this notebook. Please install it with `pip install torch torchvision`.\" ) from exc\n",
    "\n",
    "try:\n",
    "    import torchvision\n",
    "    _torchvision_version = torchvision.__version__\n",
    "except ImportError:\n",
    "    torchvision = None\n",
    "    _torchvision_version = None\n",
    "\n",
    "print(\"OpenCV     :\", cv.__version__)\n",
    "print(\"Torch      :\", torch.__version__)\n",
    "print(\"CUDA       :\", torch.cuda.is_available())\n",
    "print(\"TorchVision:\", _torchvision_version if _torchvision_version else \"not installed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b5dafc",
   "metadata": {},
   "source": [
    "## 1) Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276af2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_ROOT: /home/user/Desktop/vision_novel_application/dulana\n",
      "CHECKPOINT   : /path/to/checkpoints/unias_pcb.pt\n",
      "OUT_DIR      : /home/user/Desktop/vision_novel_application/UniAS-main/novel_application/pcb_demo_outputs\n"
     ]
    }
   ],
   "source": [
    "# === Paths (EDIT THESE) ===\n",
    "NOTEBOOK_DIR = Path.cwd()\n",
    "PROJECT_ROOT = NOTEBOOK_DIR.parent\n",
    "WEIGHTS_DIR = NOTEBOOK_DIR / \"weights\"\n",
    "\n",
    "DATASET_ROOT = Path(\"/home/user/Desktop/vision_novel_application/UniAS-main/novel_application/data\")  # <-- point to your prepared MVTec-style folder for one PCB category\n",
    "\n",
    "_def_candidates = [\n",
    "    \"visa_part2_multi_toy_ckpt.pth_best.pth\",\n",
    "    \"visa_part2_multi_toy_ckpt.pth_best.pth.tar\",\n",
    "    \"visa_part2_multi_toy_ckpt.pth.tar\",\n",
    "]\n",
    "\n",
    "_checkpoint_found = None\n",
    "if WEIGHTS_DIR.exists():\n",
    "    for pat in _def_candidates:\n",
    "        matches = sorted(WEIGHTS_DIR.rglob(pat))\n",
    "        files = [m for m in matches if m.is_file()]\n",
    "        if files:\n",
    "            _checkpoint_found = files[-1]\n",
    "            break\n",
    "\n",
    "DEFAULT_CKPT = (WEIGHTS_DIR / _def_candidates[0])\n",
    "CHECKPOINT_PATH = (_checkpoint_found or DEFAULT_CKPT).resolve()\n",
    "YAML_CFG_PATH = (PROJECT_ROOT / \"configs/visa_part2_multi.yaml\").resolve()                      # <-- UniAS config inside the repo\n",
    "OUT_DIR = (NOTEBOOK_DIR / \"pcb_demo_outputs\").resolve()                                        # results will be stored here\n",
    "\n",
    "if str(PROJECT_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(PROJECT_ROOT))\n",
    "\n",
    "if not CHECKPOINT_PATH.exists():\n",
    "    print(f\"[warn] UniAS checkpoint not found at {CHECKPOINT_PATH}.\")\n",
    "    print(\"       Update CHECKPOINT_PATH above to point to your weights file.\")\n",
    "else:\n",
    "    print(f\"[info] Using checkpoint: {CHECKPOINT_PATH}\")\n",
    "\n",
    "if not YAML_CFG_PATH.exists():\n",
    "    print(f\"[warn] YAML config missing at {YAML_CFG_PATH}.\")\n",
    "\n",
    "# Video synthesis params\n",
    "VIDEO_OUT = OUT_DIR / \"assembly_line_demo.mp4\"\n",
    "FPS = 20                 # frames per second of the output video\n",
    "FRAME_H, FRAME_W = 720, 1280         # canvas size for the video\n",
    "SEC_PER_IMAGE = 1.2      # how long each dataset image stays on screen\n",
    "CONVEYOR_SPEED_PX = 6    # px shift per frame (conveyor effect)\n",
    "STAMP_TEXT = True        # draw timestamps and labels\n",
    "\n",
    "# Inference sampling\n",
    "SAMPLE_EVERY_SEC = 1.0   # capture a frame for inference every T seconds\n",
    "THRESHOLD_MODE = \"otsu\"  # \"otsu\" | \"fixed\" | \"p95\"\n",
    "FIXED_TAU = 0.5          # used if THRESHOLD_MODE == \"fixed\"\n",
    "MIN_BLOB_AREA = 50       # remove tiny noise\n",
    "\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "(OUT_DIR / \"frames\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"masks\").mkdir(exist_ok=True)\n",
    "(OUT_DIR / \"overlays\").mkdir(exist_ok=True)\n",
    "\n",
    "print(\"DATASET_ROOT :\", DATASET_ROOT.resolve())\n",
    "print(\"YAML CONFIG  :\", YAML_CFG_PATH)\n",
    "print(\"OUT_DIR      :\", OUT_DIR)\n",
    "print(\"Running on   :\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66425509",
   "metadata": {},
   "source": [
    "## 2) Dataset utilities (VisA in MVTec-style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d66035ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_images_and_masks_mvtec_style(root: Path):\n",
    "    '''\n",
    "    Expecting structure like:\n",
    "        root/\n",
    "          train/good/*.png|jpg\n",
    "          test/good/*.png|jpg\n",
    "          test/<defect_type>/*.png|jpg\n",
    "          ground_truth/<defect_type>/*.png|jpg  (same base name as test/<defect_type>)\n",
    "    Returns list of dicts: { 'img': Path, 'mask': Path|None, 'label': 'good' or defect_type }\n",
    "    '''\n",
    "    items = []\n",
    "    test_dir = root / \"test\"\n",
    "    gt_dir   = root / \"ground_truth\"\n",
    "\n",
    "    if not test_dir.exists():\n",
    "        print(f\"[warn] No test/ directory found under {root}. Will fallback to synthetic data.\")\n",
    "        return items\n",
    "\n",
    "    for cls_dir in sorted([p for p in test_dir.iterdir() if p.is_dir()]):\n",
    "        label = cls_dir.name\n",
    "        is_good = (label.lower() == \"good\")\n",
    "\n",
    "        for imgp in sorted(cls_dir.glob(\"*.*\")):\n",
    "            if imgp.suffix.lower() not in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\", \".tiff\"]:\n",
    "                continue\n",
    "            maskp = None\n",
    "            if not is_good:\n",
    "                # ground truth mask lives at ground_truth/<label>/<same-name>\n",
    "                target_mask_dir = gt_dir / label\n",
    "                candidate = target_mask_dir / imgp.name\n",
    "                if candidate.exists():\n",
    "                    maskp = candidate\n",
    "                else:\n",
    "                    # some datasets use .png masks while images might be .jpg, try swap\n",
    "                    alt = candidate.with_suffix(\".png\")\n",
    "                    if alt.exists(): maskp = alt\n",
    "            items.append({'img': imgp, 'mask': maskp, 'label': label})\n",
    "    return items\n",
    "\n",
    "def load_image(path: Path):\n",
    "    # Robust to unicode paths:\n",
    "    img = cv.imdecode(np.fromfile(str(path), dtype=np.uint8), cv.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        img = cv.imread(str(path), cv.IMREAD_COLOR)\n",
    "    return img\n",
    "\n",
    "def put_text_multi(img, lines, org=(12,30), scale=0.8, color=(255,255,255), thickness=2):\n",
    "    x, y = org\n",
    "    for i, line in enumerate(lines):\n",
    "        cv.putText(img, str(line), (x, y + i*int(28*scale)), cv.FONT_HERSHEY_SIMPLEX, scale, color, thickness, cv.LINE_AA)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9559f025",
   "metadata": {},
   "source": [
    "## 3) Build an **assembly-line** demo video from dataset images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4773975",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def letterbox_pad(img, target_h, target_w):\n",
    "    h, w = img.shape[:2]\n",
    "    scale = min(target_w / w, target_h / h)\n",
    "    nw, nh = int(w*scale), int(h*scale)\n",
    "    resized = cv.resize(img, (nw, nh), interpolation=cv.INTER_AREA)\n",
    "    canvas = np.zeros((target_h, target_w, 3), dtype=np.uint8)\n",
    "    y0 = (target_h - nh) // 2\n",
    "    x0 = (target_w - nw) // 2\n",
    "    canvas[y0:y0+nh, x0:x0+nw] = resized\n",
    "    return canvas\n",
    "\n",
    "def draw_conveyor_belt(frame, offset_px=0, lane_h=200):\n",
    "    '''Simple gray belt with dashed lines to simulate movement'''\n",
    "    h, w = frame.shape[:2]\n",
    "    y0 = h - lane_h\n",
    "    frame[y0:h,:] = (40, 40, 40)\n",
    "    dash_w = 80\n",
    "    gap_w  = 40\n",
    "    x = (-offset_px) % (dash_w + gap_w)\n",
    "    while x < w:\n",
    "        x_end = min(w, x + dash_w)\n",
    "        cv.rectangle(frame, (x, y0 + lane_h//2 - 4), (x_end, y0 + lane_h//2 + 4), (180,180,180), -1)\n",
    "        x += dash_w + gap_w\n",
    "    return frame\n",
    "\n",
    "def build_synthetic_video(out_path: Path, fps=20, frame_h=720, frame_w=1280, secs=12):\n",
    "    fourcc = cv.VideoWriter_fourcc(*\"mp4v\")\n",
    "    vw = cv.VideoWriter(str(out_path), fourcc, fps, (frame_w, frame_h))\n",
    "    nframes = secs * fps\n",
    "    offset = 0\n",
    "    for i in range(nframes):\n",
    "        frame = np.zeros((frame_h, frame_w, 3), dtype=np.uint8)\n",
    "        # moving rectangles (fake PCBs)\n",
    "        for k in range(5):\n",
    "            x = int((i*7 + k*240) % (frame_w+200)) - 200\n",
    "            y = frame_h - 250 - k*40\n",
    "            cv.rectangle(frame, (x, y), (x+200, y+120), (20+40*k, 120, 40), -1)\n",
    "        frame = draw_conveyor_belt(frame, offset_px=offset, lane_h=180)\n",
    "        offset += 8\n",
    "        if i % 30 == 0:\n",
    "            put_text_multi(frame, [\"Synthetic Demo (no dataset found)\"], org=(20,40), scale=1.0)\n",
    "        vw.write(frame)\n",
    "    vw.release()\n",
    "    print(f\"[ok] Wrote synthetic video: {out_path.resolve()}\")\n",
    "    return str(out_path)\n",
    "\n",
    "def build_video_from_dataset(root: Path, out_path: Path, fps=20, frame_h=720, frame_w=1280,\n",
    "                             sec_per_image=1.2, conveyor_speed_px=6, stamp=True):\n",
    "    items = find_images_and_masks_mvtec_style(root)\n",
    "    if len(items) == 0:\n",
    "        print(\"[warn] No dataset images found. Creating a synthetic demo video instead.\")\n",
    "        return build_synthetic_video(out_path, fps, frame_h, frame_w)\n",
    "\n",
    "    # Shuffle but ensure some alternation of good/anomaly if possible\n",
    "    random.seed(42)\n",
    "    good = [d for d in items if d['label'] == 'good']\n",
    "    bad  = [d for d in items if d['label'] != 'good']\n",
    "    seq  = []\n",
    "    while good or bad:\n",
    "        if bad:\n",
    "            seq.append(bad.pop(0))\n",
    "        if good:\n",
    "            seq.append(good.pop(0))\n",
    "    if len(seq) < 10:\n",
    "        seq = (seq * (10 // max(1, len(seq)) + 1))[:10]\n",
    "\n",
    "    fourcc = cv.VideoWriter_fourcc(*\"mp4v\")\n",
    "    vw = cv.VideoWriter(str(out_path), fourcc, fps, (frame_w, frame_h))\n",
    "    frames_per_image = max(1, int(round(sec_per_image * fps)))\n",
    "\n",
    "    conveyor_offset = 0\n",
    "\n",
    "    for idx, it in enumerate(seq):\n",
    "        img = load_image(it['img'])\n",
    "        if img is None:\n",
    "            continue\n",
    "        canvas = letterbox_pad(img, frame_h, frame_w)\n",
    "\n",
    "        for _ in range(frames_per_image):\n",
    "            frame = canvas.copy()\n",
    "            frame = draw_conveyor_belt(frame, offset_px=conveyor_offset, lane_h=180)\n",
    "            conveyor_offset += conveyor_speed_px\n",
    "\n",
    "            if stamp:\n",
    "                now = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "                txt = [f\"VisA PCB Demo  |  {it['label'].upper()}  |  {it['img'].name}\", f\"{now}\"]\n",
    "                put_text_multi(frame, txt, org=(18,36), scale=0.9, color=(255,255,255), thickness=2)\n",
    "\n",
    "            vw.write(frame)\n",
    "\n",
    "    vw.release()\n",
    "    print(f\"[ok] Wrote video: {out_path.resolve()}\")\n",
    "    return str(out_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "440c1e22",
   "metadata": {},
   "source": [
    "### \u25b6\ufe0f Create the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fd26b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[warn] No test/ directory found under /home/user/Desktop/vision_novel_application/dulana. Will fallback to synthetic data.\n",
      "[warn] No dataset images found. Creating a synthetic demo video instead.\n",
      "[ok] Wrote synthetic video: /home/user/Desktop/vision_novel_application/UniAS-main/novel_application/pcb_demo_outputs/assembly_line_demo.mp4\n",
      "Video at: pcb_demo_outputs/assembly_line_demo.mp4\n"
     ]
    }
   ],
   "source": [
    "\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "VIDEO_OUT = OUT_DIR / \"assembly_line_demo.mp4\"\n",
    "video_path = build_video_from_dataset(DATASET_ROOT, VIDEO_OUT, fps=FPS, frame_h=FRAME_H, frame_w=FRAME_W,\n",
    "                                      sec_per_image=SEC_PER_IMAGE, conveyor_speed_px=CONVEYOR_SPEED_PX,\n",
    "                                      stamp=STAMP_TEXT)\n",
    "print(\"Video at:\", video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ec3ee1",
   "metadata": {},
   "source": [
    "## 4) Inference on the video (sample every T seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1b97901",
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_H, INPUT_W = 224, 224\n",
    "PIXEL_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "PIXEL_STD = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "def preprocess_bgr_to_model(img_bgr):\n",
    "    img_rgb = cv.cvtColor(img_bgr, cv.COLOR_BGR2RGB).astype(np.float32) / 255.0\n",
    "    img_res = cv.resize(img_rgb, (INPUT_W, INPUT_H), interpolation=cv.INTER_AREA)\n",
    "    img_norm = (img_res - PIXEL_MEAN) / PIXEL_STD\n",
    "    ten = torch.from_numpy(img_norm).permute(2, 0, 1).unsqueeze(0)  # [1,3,H,W]\n",
    "    return ten\n",
    "\n",
    "def try_build_unias_from_yaml(yaml_path: Path, device: str = \"cpu\"):\n",
    "    try:\n",
    "        from unias.config import get_cfg\n",
    "        from unias.modeling import build_model\n",
    "        cfg = get_cfg()\n",
    "        cfg.merge_from_file(str(yaml_path))\n",
    "        model = build_model(cfg).to(device)\n",
    "        model.eval()\n",
    "        print(\"[info] Built UniAS model via YAML config.\")\n",
    "        return model, cfg\n",
    "    except Exception as exc:\n",
    "        print(\"[warn] Could not build UniAS model from YAML; falling back. Reason:\", exc)\n",
    "        return None, None\n",
    "\n",
    "class FallbackTinyAnom(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv2d(3, 1, kernel_size=3, padding=1)\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "def _strip_module_prefix(state_dict):\n",
    "    return {(k[7:] if k.startswith(\"module.\") else k): v for k, v in state_dict.items()}\n",
    "\n",
    "def _resolve_ckpt_path(pathlike: Path) -> Path:\n",
    "    path = Path(pathlike)\n",
    "    if path.is_dir():\n",
    "        # handle checkpoints that were extracted from torch zip archives\n",
    "        data_pkl = path / \"data.pkl\"\n",
    "        if data_pkl.exists():\n",
    "            print(f\"[info] Loading extracted checkpoint from {data_pkl}\")\n",
    "            return data_pkl\n",
    "        inner = sorted(p for p in path.rglob(\"*.pth\") if p.is_file())\n",
    "        if inner:\n",
    "            print(f\"[info] Loading inner checkpoint file {inner[-1]}\")\n",
    "            return inner[-1]\n",
    "        raise FileNotFoundError(f\"No checkpoint file found inside directory {path}\")\n",
    "    return path\n",
    "\n",
    "def load_unias_weights(model: torch.nn.Module, ckpt_path: Path, device: str = \"cpu\"):\n",
    "    ckpt_file = _resolve_ckpt_path(ckpt_path)\n",
    "    ckpt = torch.load(str(ckpt_file), map_location=device)\n",
    "    if isinstance(ckpt, dict):\n",
    "        if isinstance(ckpt.get(\"state_dict\"), dict):\n",
    "            state = ckpt[\"state_dict\"]\n",
    "        elif isinstance(ckpt.get(\"model\"), dict):\n",
    "            state = ckpt[\"model\"]\n",
    "        else:\n",
    "            tensor_like = {k: v for k, v in ckpt.items() if isinstance(v, torch.Tensor)}\n",
    "            state = tensor_like if tensor_like else ckpt\n",
    "    else:\n",
    "        state = ckpt\n",
    "\n",
    "    state = _strip_module_prefix(state)\n",
    "    missing, unexpected = model.load_state_dict(state, strict=False)\n",
    "    print(f\"[load] missing keys: {len(missing)}, unexpected keys: {len(unexpected)}\")\n",
    "    if missing:\n",
    "        print(\"   first missing:\", missing[:8])\n",
    "    if unexpected:\n",
    "        print(\"   first unexpected:\", unexpected[:8])\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_model(ckpt_path: Path, yaml_path: Path = None, device: str = \"cpu\"):\n",
    "    model, _ = (None, None)\n",
    "    if yaml_path is not None and yaml_path.exists():\n",
    "        model, _ = try_build_unias_from_yaml(yaml_path, device=device)\n",
    "    elif yaml_path is not None:\n",
    "        print(f\"[warn] YAML config not found at {yaml_path}; skipping repo builder.\")\n",
    "\n",
    "    if model is None:\n",
    "        model = FallbackTinyAnom().to(device)\n",
    "        print(\"[info] Using fallback tiny anomaly head (demo only).\")\n",
    "\n",
    "    if ckpt_path is not None and Path(ckpt_path).exists():\n",
    "        load_unias_weights(model, Path(ckpt_path), device=device)\n",
    "        print(f\"[info] Loaded weights from {ckpt_path}\")\n",
    "    else:\n",
    "        print(f\"[warn] Checkpoint missing at {ckpt_path}; running with random weights.\")\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def model_forward_to_anomap(model: torch.nn.Module, frame_bgr, device: str = \"cpu\"):\n",
    "    tin = preprocess_bgr_to_model(frame_bgr).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(tin)\n",
    "\n",
    "    if isinstance(out, dict):\n",
    "        if \"anomaly\" in out:\n",
    "            logits = out[\"anomaly\"]\n",
    "        elif \"pred_masks\" in out:\n",
    "            logits = out[\"pred_masks\"]\n",
    "        else:\n",
    "            logits = next(v for v in out.values() if torch.is_tensor(v))\n",
    "    else:\n",
    "        logits = out\n",
    "\n",
    "    if logits.ndim != 4:\n",
    "        raise RuntimeError(f\"Unexpected output shape: {tuple(logits.shape)} (expect BxCxHxW)\")\n",
    "\n",
    "    score = torch.sigmoid(logits)\n",
    "    if score.shape[1] == 1:\n",
    "        anomap = score[0, 0].detach().cpu().numpy()\n",
    "    else:\n",
    "        anomap = score[0].amax(dim=0).detach().cpu().numpy()\n",
    "\n",
    "    H, W = frame_bgr.shape[:2]\n",
    "    anomap_up = cv.resize(anomap, (W, H), interpolation=cv.INTER_LINEAR)\n",
    "    return anomap_up\n",
    "\n",
    "def anomaly_to_mask(anomap, mode='otsu', tau=0.5, min_blob=50):\n",
    "    a = (anomap * 255.0).astype(np.uint8)\n",
    "    if mode == 'otsu':\n",
    "        _, m = cv.threshold(a, 0, 255, cv.THRESH_BINARY + cv.THRESH_OTSU)\n",
    "    elif mode == 'p95':\n",
    "        thr = np.percentile(a, 95)\n",
    "        _, m = cv.threshold(a, thr, 255, cv.THRESH_BINARY)\n",
    "    else:\n",
    "        thr = int(round(tau * 255))\n",
    "        _, m = cv.threshold(a, thr, 255, cv.THRESH_BINARY)\n",
    "    m = cv.morphologyEx(m, cv.MORPH_OPEN, np.ones((3, 3), np.uint8))\n",
    "    if min_blob > 0:\n",
    "        cnts, _ = cv.findContours(m, cv.RETR_EXTERNAL, cv.CHAIN_APPROX_SIMPLE)\n",
    "        keep = np.zeros_like(m)\n",
    "        for c in cnts:\n",
    "            if cv.contourArea(c) >= min_blob:\n",
    "                cv.drawContours(keep, [c], -1, 255, thickness=cv.FILLED)\n",
    "        m = keep\n",
    "    return m\n",
    "\n",
    "def overlay_mask(bgr, mask, alpha=0.5):\n",
    "    color = np.zeros_like(bgr)\n",
    "    color[:, :, 2] = 255  # red overlay\n",
    "    mask3 = cv.cvtColor(mask, cv.COLOR_GRAY2BGR)\n",
    "    overlay = np.where(mask3 > 0, cv.addWeighted(bgr, 1 - alpha, color, alpha, 0), bgr)\n",
    "    return overlay\n",
    "\n",
    "def run_interval_inference(video_path: Path, out_dir: Path, interval_sec=1.0, device='cpu',\n",
    "                           threshold_mode='otsu', fixed_tau=0.5, min_blob_area=50, show_preview=False):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(exist_ok=True, parents=True)\n",
    "    (out_dir / 'frames').mkdir(exist_ok=True)\n",
    "    (out_dir / 'masks').mkdir(exist_ok=True)\n",
    "    (out_dir / 'overlays').mkdir(exist_ok=True)\n",
    "\n",
    "    csv_path = out_dir / 'inspection_log.csv'\n",
    "    new_csv = not csv_path.exists()\n",
    "    fcsv = open(csv_path, 'a', newline='')\n",
    "    wr = csv.writer(fcsv)\n",
    "    if new_csv:\n",
    "        wr.writerow(['timestamp', 'frame_idx', 'anomaly_pixels', 'anomaly_ratio', 'frame_path', 'mask_path', 'overlay_path'])\n",
    "\n",
    "    cap = cv.VideoCapture(str(video_path))\n",
    "    if not cap.isOpened():\n",
    "        raise RuntimeError(f'Cannot open video: {video_path}')\n",
    "    fps = cap.get(cv.CAP_PROP_FPS) or 25.0\n",
    "    sample_every_n = max(1, int(round(interval_sec * fps)))\n",
    "    print(f\"[info] Sampling every {interval_sec}s (~{sample_every_n} frames), video FPS {fps:.1f}\")\n",
    "\n",
    "    model = load_model(CHECKPOINT_PATH, yaml_path=YAML_CFG_PATH, device=device)\n",
    "\n",
    "    idx = 0\n",
    "    last = -sample_every_n\n",
    "    while True:\n",
    "        ok, frame = cap.read()\n",
    "        if not ok:\n",
    "            break\n",
    "\n",
    "        if idx - last >= sample_every_n:\n",
    "            last = idx\n",
    "            anomap = model_forward_to_anomap(model, frame, device=device)\n",
    "            mask = anomaly_to_mask(anomap, mode=threshold_mode, tau=fixed_tau, min_blob=min_blob_area)\n",
    "\n",
    "            anomaly_pixels = int((mask > 0).sum())\n",
    "            anomaly_ratio = anomaly_pixels / (mask.size + 1e-6)\n",
    "\n",
    "            now = datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "            f_frame = out_dir / 'frames' / f\"{now}.png\"\n",
    "            f_mask = out_dir / 'masks' / f\"{now}.png\"\n",
    "            f_ov = out_dir / 'overlays' / f\"{now}.png\"\n",
    "\n",
    "            overlay = overlay_mask(frame, mask)\n",
    "            cv.imwrite(str(f_frame), frame)\n",
    "            cv.imwrite(str(f_mask), mask)\n",
    "            cv.imwrite(str(f_ov), overlay)\n",
    "\n",
    "            wr.writerow([now, idx, anomaly_pixels, f\"{anomaly_ratio:.6f}\", str(f_frame), str(f_mask), str(f_ov)])\n",
    "            fcsv.flush()\n",
    "\n",
    "            if show_preview:\n",
    "                disp = overlay.copy()\n",
    "                cv.putText(disp, f\"Anomaly px: {anomaly_pixels} ({anomaly_ratio * 100:.3f}%)\", (12, 30),\n",
    "                           cv.FONT_HERSHEY_SIMPLEX, 0.8, (255, 255, 255), 2, cv.LINE_AA)\n",
    "                cv.imshow(\"Operator Preview\", disp)\n",
    "                if (cv.waitKey(1) & 0xFF) == 27:  # ESC\n",
    "                    break\n",
    "        idx += 1\n",
    "\n",
    "    cap.release()\n",
    "    fcsv.close()\n",
    "    cv.destroyAllWindows()\n",
    "    print(f\"[ok] Inference complete. Log: {csv_path.resolve()}\")\n",
    "    return str(csv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a71f2c",
   "metadata": {},
   "source": [
    "### \u25b6\ufe0f Run interval inference on the generated video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0fa7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log = run_interval_inference(\n",
    "    video_path=VIDEO_OUT,\n",
    "    out_dir=OUT_DIR,\n",
    "    interval_sec=SAMPLE_EVERY_SEC,\n",
    "    device=DEVICE,\n",
    "    threshold_mode=THRESHOLD_MODE,\n",
    "    fixed_tau=FIXED_TAU,\n",
    "    min_blob_area=MIN_BLOB_AREA,\n",
    "    show_preview=False  # set True to see a live window\n",
    ")\n",
    "print(\"CSV log at:\", csv_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39471b7f",
   "metadata": {},
   "source": [
    "## 5) Quick look at saved overlays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "950329aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from IPython.display import Video, display\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Try inline playback\n",
    "try:\n",
    "    display(Video(filename=str(VIDEO_OUT), embed=True))\n",
    "except Exception as e:\n",
    "    print(\"Inline video preview may not work here:\", e)\n",
    "\n",
    "# Show a few overlays\n",
    "samples = sorted((OUT_DIR / 'overlays').glob('*.png'))[:6]\n",
    "print(f\"Showing {len(samples)} overlays...\")\n",
    "for p in samples:\n",
    "    img = cv.imread(str(p), cv.IMREAD_COLOR)\n",
    "    if img is None: \n",
    "        continue\n",
    "    img = cv.cvtColor(img, cv.COLOR_BGR2RGB)\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.imshow(img); plt.axis('off'); plt.title(p.name)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2deec6",
   "metadata": {},
   "source": [
    "\n",
    "## Notes & Tips\n",
    "- **Resize scheme**: The video builder uses letterboxing to keep aspect ratio. If your model expects a fixed input size, consider resizing frames right before inference.\n",
    "- **Thresholds**: Start with `otsu`. If masks are noisy, try `p95`. Use `fixed` only if you\u2019ve calibrated scores from your training setup.\n",
    "- **Save-on-alert**: To save disk, wrap the save block inside `if anomaly_ratio > 0.001:` (or a threshold suited to your line).\n",
    "- **Real camera later**: Replace the `VIDEO_OUT` path with an RTSP/USB source handled by OpenCV and reuse the same inference cell.\n",
    "- **Latency**: For live demos, export to ONNX/TensorRT and replace `load_model()` with an engine loader.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}